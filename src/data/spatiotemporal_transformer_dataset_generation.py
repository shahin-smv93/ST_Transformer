# -*- coding: utf-8 -*-
"""Spatiotemporal_transformer_dataset_generation.ipynb

Automatically generated by Colab.

"""

import numpy as np
import os
import glob
import pandas as pd
import argparse
from sklearn.model_selection import train_test_split
import torch
from torch.utils.data import Dataset, DataLoader
from typing import List
from scipy.fftpack import fft
import pytorch_lightning as pl
import sys
from scipy.signal import welch

def generate_spectral_welch(data, sf, nperseg, num_dominant_frequencies):
    frequencies, psd_values = welch(data, sf, nperseg=nperseg)
    dominant_indices = np.argsort(psd_values)[-num_dominant_frequencies:]
    dominant_frequencies = frequencies[dominant_indices]
    dominant_psd_values = psd_values[dominant_indices]
    sorted_indices = np.argsort(dominant_frequencies)
    dominant_frequencies = dominant_frequencies[sorted_indices]
    dominant_psd_values = dominant_psd_values[sorted_indices]
    return dominant_frequencies, dominant_psd_values

class SimulationTimeSeries:
    def __init__(
        self,
        data_path: str = None,
        data=None,
        time_col_name: str = 'Time',
        start_time=0.0,
        time_step_df: float = 1.0,
        time_step_feature: float = 1e-3,
        idx_start_train: int = 0,
        idx_end_train: int = 6084,
        idx_start_val: int = 6090,
        idx_end_val: int =7600,
        have_test: bool = False,
        idx_start_test: int = None,
        idx_end_test: int = None,
        num_dominant_frequencies: int = 1,
        use_cycles: bool = True,
        nperseg: int = 1024,
    ):
        self.time_step_df = time_step_df
        self.start_time = start_time
        self.idx_start_train = idx_start_train
        self.idx_end_train = idx_end_train
        self.idx_start_val = idx_start_val
        self.idx_end_val = idx_end_val
        self.time_step_feature = time_step_feature
        self.num_dominant_frequencies = num_dominant_frequencies
        self.use_cycles = use_cycles
        self.time_col_name = time_col_name
        self.nperseg = nperseg

        if data_path:
            self.data = self.load_data(data_path)
        else:
            self.data = data

        if have_test:
            assert idx_start_test is not None
            assert idx_end_test is not None
            self.idx_start_test = idx_start_test
            self.idx_end_test = idx_end_test
        else:
            self.idx_start_test = None
            self.idx_end_test = None

        self.df = self._create_timeseries_dataframe(self.data)

        if self.use_cycles:
            self.generate_aggregated_spectral_features(self.df)
        else:
            self.add_simple_timesteps(self.df)

        self._identify_columns()
        self.split_data()

    def load_data(self, data_path):
        data = np.load(data_path)
        return data['dataset']

    def _create_timeseries_dataframe(self, data):
        num_timesteps, num_sensors = data.shape
        time_vals = np.arange(self.start_time, self.start_time + num_timesteps * self.time_step_df, self.time_step_df)[:num_timesteps]

        df = pd.DataFrame(data, columns=[f"Sensor_{i+1}" for i in range(num_sensors)])
        df.insert(0, self.time_col_name, time_vals)
        return df

    def split_data(self):
        df = self.df.copy()

        train_df = df.iloc[self.idx_start_train:self.idx_end_train]
        val_df = df.iloc[self.idx_start_val:self.idx_end_val]

        if self.idx_start_test is not None and self.idx_end_test is not None:
            test_df = df.iloc[self.idx_start_test:self.idx_end_test]
        else:
            test_df = pd.DataFrame()

        self.train_df = train_df
        self.val_df = val_df
        self.test_df = test_df

    def generate_spectral_density(self, data, sf):
        frequencies, psd_values = welch(data, sf, nperseg=self.nperseg)
        dominant_indices = np.argsort(psd_values)[-self.num_dominant_frequencies:]
        dominant_frequencies = frequencies[dominant_indices]
        return dominant_frequencies

    def generate_time_based_features(self, df, dominant_freqs):
        num_timesteps = len(df)
        t = np.arange(0, num_timesteps * self.time_step_feature, self.time_step_feature)

        if len(t) > num_timesteps:
            t = t[:num_timesteps]

        for i, freq in enumerate(dominant_freqs):
            df[f'Sin_Time_{i}'] = np.sin((2 * np.pi * freq * t))
            df[f'Cos_Time_{i}'] = np.cos((2 * np.pi * freq * t))

    def generate_aggregated_spectral_features(self, df):
        sf = 1 / self.time_step_feature
        mean_sensor_data = df.iloc[:, 1:].mean(axis=1).values
        dominant_freqs = self.generate_spectral_density(mean_sensor_data, sf)

        self.generate_time_based_features(df, dominant_freqs)

    def add_simple_timesteps(self, df):
        N = len(df)
        t = np.arange(N) * self.time_step_df + self.start_time
        t_norm = (t - t[0]) / (t[-1] - t[0])
        df['Time_Index'] = t_norm

    def _identify_columns(self):
        if self.use_cycles:
            time_related_cols = [col for col in self.df.columns if '_Time' in col]
        else:
            time_related_cols = ['Time_Index']
        self.target_cols = [col for col in self.df.columns if col not in time_related_cols]
        self.target_cols.remove(self.time_col_name)
        self.time_cols = time_related_cols
        self.exo_cols = [
            col for col in self.df.columns if col not in self.time_cols and not col.startswith('Sensor_')
        ]
        self.exo_cols.remove(self.time_col_name)

    def get_slice(self, split, start, stop, skip):
        assert split in ["train", "val", "test"]
        if split == "train":
            data = self.train_df
        elif split == "val":
            data = self.val_df
        else:
            data = self.test_df

        return data.iloc[start:stop:skip]

    @property
    def train_data(self):
        return self.train_df

    @property
    def val_data(self):
        return self.val_df

    @property
    def test_data(self):
        return self.test_df

    def length(self, split):
        data = {
            "train": len(self.train_data),
            "val": len(self.val_data),
            "test": len(self.test_data),
        }[split]
        return data

    @classmethod
    def add_cli(cls, parser):
        parser.add_argument('--data_path', type=str, help='Path to the data file')

class SimulationTorchDataset(Dataset):
  def __init__(self, time_series: SimulationTimeSeries,
               split: str = "train", context_len: int = 128,
               target_len: int = 32, time_resolution: int = 1):
    super().__init__()

    assert split in ["train", "val", "test"]
    self.split = split
    self.series = time_series
    self.context_len = context_len
    self.target_len = target_len
    self.time_resolution = time_resolution

    self._slice_start_points = [
        i for i in range(
            0, self.series.length(split)
            + time_resolution * (-target_len - context_len)
            +1,
        )
    ]

  def __len__(self):
    return len(self._slice_start_points)

  def _torch(self, *dfs):
    return tuple(torch.from_numpy(x.values).float() for x in dfs)

  def __getitem__(self, idx):
    start = self._slice_start_points[idx]
    series_slice = self.series.get_slice(
        self.split,
        start=start,
        stop=start + self.time_resolution * (self.context_len + self.target_len),
        skip=self.time_resolution,
    )
    #print(f"Columns before dropping Time: {series_slice.columns}")
    #print("time_col_name", self.series.time_col_name)
    series_slice = series_slice.drop(columns=[self.series.time_col_name])
    #print(f"Columns after dropping Time: {series_slice.columns}")

    context_slice = series_slice.iloc[:self.context_len]
    target_slice = series_slice.iloc[self.context_len:]

    #print("time_cols\n---------", self.series.time_cols)
    context_x = context_slice[self.series.time_cols]
    #print('runs up to this point')
    context_y = context_slice[self.series.target_cols + self.series.exo_cols]

    target_x = target_slice[self.series.time_cols]
    target_y = target_slice[self.series.target_cols]

    return self._torch(context_x, context_y, target_x, target_y)

  @classmethod
  def add_cli(cls, parser):
    parser.add_argument(
        "--context_len", type=int, default=128, help="Length of context length"
    )
    parser.add_argument(
        "--target_len", type=int, default=32, help="Length of target length"
    )
    parser.add_argument(
        "--time_resolution", type=int, default=1, help="Time resolution"
    )

class DataModule(pl.LightningDataModule):
    def __init__(self, dataset, dataset_kwargs: dict, batch_size: int, num_workers: int, prediction: bool = False, prediction_dataset: str = "val", overfit: bool = False):
        super().__init__()
        self.dataset = dataset
        self.batch_size = batch_size
        self.num_workers = num_workers
        self.prediction = prediction
        self.overfit = overfit
        if self.prediction:
            assert prediction_dataset in ["val", "test"]
            self.prediction_dataset = prediction_dataset
        else:
            self.prediction_dataset = None
        if "split" in dataset_kwargs.keys():
            del dataset_kwargs['split']
        self.dataset_kwargs = dataset_kwargs


    def train_dataloader(self, shuffle=False):
       return self._make_dataloader("train", shuffle=False)

    def val_dataloader(self, shuffle=False):
        return self._make_dataloader("val", shuffle=False)

    def test_dataloader(self, shuffle=False):
        return self._make_dataloader("test", shuffle=False)

    def predict_dataloader(self, shuffle=False):
        if self.prediction:
            return self._make_dataloader(self.prediction_dataset, shuffle=False)
        else:
            return None

    def _make_dataloader(self, split, shuffle=False):
        if self.overfit:
            split = "train"
            shuffle = False
        return DataLoader(
            dataset=self.dataset(**self.dataset_kwargs, split=split),
            batch_size=self.batch_size,
            num_workers=self.num_workers,
            shuffle=shuffle
        )

    @classmethod
    def add_cli(cls, parser):
        parser.add_argument('--batch_size', type=int, default=64)
        parser.add_argument('--num_workers', type=int, default=6)
        parser.add_argument('--prediction', action='store_true')
        parser.add_argument('--prediction_dataset', type=str, default='val', choices=['val', 'test'])
        parser.add_argument('--overfit', action='store_true')


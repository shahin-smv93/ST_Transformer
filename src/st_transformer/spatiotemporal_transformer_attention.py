# -*- coding: utf-8 -*-
"""Spatiotemporal_transformer_attention.ipynb

Automatically generated by Colab.

"""

#!pip install performer-pytorch

from performer_pytorch import FastAttention as _FastAttention
import math
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from functools import partial

#!pip install einops
from einops import repeat

class PerformerAttention(_FastAttention):
  def __init__(self, dim_heads=None, ortho_scaling=0.0, redraw_interval=1000,
               mask_flag=False, kernel='softmax'):
    super().__init__(
        dim_heads=dim_heads,
        nb_features=max(100, int(dim_heads * math.log(dim_heads))),
        ortho_scaling=ortho_scaling,
        causal=mask_flag,
        generalized_attention=kernel=="relu",
        kernel_fn=nn.ReLU() if kernel=="relu" else None
    )

    self.redraw_interval = redraw_interval
    self.register_buffer('last_projection_matrix_redraw', torch.tensor(0))

  def forward(self, q, k, v, attention_mask, output_attention=False):
    if self.training:
      if self.last_projection_matrix_redraw >= self.redraw_interval:
        self.redraw_projection_matrix(q.device)
        self.last_projection_matrix_redraw.zero_()
      else:
        self.last_projection_matrix_redraw += 1

    q = q.transpose(1, 2)
    k = k.transpose(1, 2)
    v = v.transpose(1, 2)

    output = super().forward(q, k, v)
    output = output.transpose(1, 2)

    return output, None

class AttentionClass(nn.Module):
  def __init__(self, d_model, d_q_k, d_v, n_heads, dropout, attention):
    super(AttentionClass, self).__init__()

    self.n_heads = n_heads
    self.inner_attention = attention()
    self.q_proj = nn.Linear(d_model, d_q_k * n_heads)
    self.k_proj = nn.Linear(d_model, d_q_k * n_heads)
    self.v_proj = nn.Linear(d_model, d_v * n_heads)
    self.out_proj = nn.Linear(d_v * n_heads, d_model)
    self.dropout = nn.Dropout(dropout)

  def forward(self, q, k, v, attention_mask, output_attention=False):
    H = self.n_heads
    B, lenQ, _ = q.shape
    _, lenK, _ = k.shape

    q = self.dropout(self.q_proj(q)).view(B, lenQ, H, -1)
    k = self.dropout(self.k_proj(k)).view(B, lenK, H, -1)
    v = self.dropout(self.v_proj(v)).view(B, lenK, H, -1)

    output, attn = self.inner_attention(q, k, v, attention_mask, output_attention)

    if output_attention and attn is None:
      new_v = (
          torch.eye(lenK).unsqueeze(0).repeat(B, 1, 1).unsqueeze(2).to(v.device)
      )
      with torch.no_grad():
        attn, _ = self.inner_attention(
            q, k, v=new_v, attention_mask=attention_mask)
        attn = attn.transpose(2, 1)

    output = output.view(B, lenQ, -1)

    if not output_attention:
      assert attn is None

    output = self.out_proj(output)

    return output, attn

def create_performer_attention(d_model, d_q_k, d_v, n_heads, dropout_qkv, performer_attention_kernel, redraw_interval):
    return AttentionClass(
        d_model=d_model,
        d_q_k=d_q_k,
        d_v=d_v,
        n_heads=n_heads,
        dropout=dropout_qkv,
        attention=partial(
            PerformerAttention,
            dim_heads=d_q_k,
            kernel=performer_attention_kernel,
            redraw_interval=redraw_interval,
        )
    )
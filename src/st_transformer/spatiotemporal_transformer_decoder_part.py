# -*- coding: utf-8 -*-
"""Spatiotemporal_transformer_Decoder_part.ipynb

Automatically generated by Colab.

"""

#!pip install einops
from einops import rearrange, repeat
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

#!pip install performer-pytorch

from .spatiotemporal_transformer_extralayers import (
    localize, reverse_localize, Windowing, ReverseWindowing, SelfMaskingSeq, CrossMaskingSeq, Normalization
)

class DecoderLayer(nn.Module):
  def __init__(self, d_model, d_y_target, d_y_context,
              local_self_attention, global_self_attention,
              local_cross_attention, global_cross_attention,
              d_q_k: int, d_v: int, n_heads: int, dropout_qkv: float,
              dropout_ff: float = 0.1,
              dropout_attn: float = 0.0, windows=1, window_offset=0,
              d_ff: int = None, normalization_type='batchnorm', activation='relu'):
    super().__init__()

    if d_ff is None:
      d_ff = 4 * d_model
    else:
      d_ff = d_ff

    self.local_self_attention = local_self_attention
    self.global_self_attention = global_self_attention
    self.global_cross_attention = global_cross_attention
    if local_cross_attention is not None and d_y_context != d_y_target:
      assert d_y_context > d_y_target

    self.local_cross_attention = local_cross_attention

    self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)
    self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)

    self.norm1 = Normalization(d_model=d_model, method=normalization_type)
    self.norm2 = Normalization(d_model=d_model, method=normalization_type)
    self.norm3 = Normalization(d_model=d_model, method=normalization_type)
    self.norm4 = Normalization(d_model=d_model, method=normalization_type)
    self.norm5 = Normalization(d_model=d_model, method=normalization_type)

    self.dropout_ff = nn.Dropout(dropout_ff)
    self.dropout_attn = nn.Dropout(dropout_attn)

    self.activation = F.relu if activation == 'relu' else F.gelu

    self.d_y_target = d_y_target
    self.d_y_context = d_y_context
    self.windows = windows
    self.window_offset = window_offset

  def forward(self, x, encoder_output, self_mask_seq=None, cross_mask_seq=None,
              cross_attention=False):

    attention_ = None
    if self.local_self_attention:
      assert self_mask_seq is None
      x1 = self.norm1(x)
      x1 = localize(x1, self.d_y_target)
      x1, _ = self.local_self_attention(x1, x1, x1, attention_mask=self_mask_seq)
      x1 = reverse_localize(x1, self.d_y_target)
      x = x + self.dropout_attn(x1)

    if self.global_self_attention:
      x1 = self.norm2(x)
      x1 = Windowing(x1, num_windows=self.windows, window_offset=self.window_offset, divide_into=self.d_y_target)
      self_mask_seq = Windowing(self_mask_seq, num_windows=self.windows, window_offset=self.window_offset, divide_into=self.d_y_target)
      x1, _ = self.global_self_attention(x1, x1, x1, attention_mask=SelfMaskingSeq(self_mask_seq))
      x1 = ReverseWindowing(x1, num_windows=self.windows, window_offset=self.window_offset, divide_into=self.d_y_target)
      self_mask_seq = ReverseWindowing(self_mask_seq, num_windows=self.windows, window_offset=self.window_offset, divide_into=self.d_y_target)
      x = x + self.dropout_attn(x1)

    if self.local_cross_attention:
      assert cross_mask_seq is None
      x1 = self.norm3(x)
      bs, *_ = x1.shape
      x1 = localize(x1, self.d_y_target)
      encoder_output_localize = localize(encoder_output, self.d_y_context)[: self.d_y_target * bs]
      x1, _ = self.local_cross_attention(x1, encoder_output_localize, encoder_output_localize, attention_mask=cross_mask_seq)
      x1 = reverse_localize(x1, self.d_y_target)
      x = x + self.dropout_attn(x1)

    if self.global_cross_attention:
      x1 = self.norm4(x)
      x1 = Windowing(x1, num_windows=self.windows, window_offset=self.window_offset, divide_into=self.d_y_target)
      encoder_output = Windowing(encoder_output, num_windows=self.windows, window_offset=self.window_offset, divide_into=self.d_y_context)
      cross_mask_seq = Windowing(cross_mask_seq, num_windows=self.windows, window_offset=self.window_offset, divide_into=self.d_y_context)
      x1, attention_ = self.global_cross_attention(x1, encoder_output, encoder_output, attention_mask=CrossMaskingSeq(self_mask_seq, cross_mask_seq), output_attention=cross_attention)
      encoder_output = ReverseWindowing(encoder_output, num_windows=self.windows, window_offset=self.window_offset, divide_into=self.d_y_context)
      cross_mask_seq = ReverseWindowing(cross_mask_seq, num_windows=self.windows, window_offset=self.window_offset, divide_into=self.d_y_context)
      x1 = ReverseWindowing(x1, num_windows=self.windows, window_offset=self.window_offset, divide_into=self.d_y_target)
      x = x + self.dropout_attn(x1)

    x1 = self.norm5(x)
    x1 = self.dropout_ff(self.activation(self.conv1(x1.transpose(-1, -2))))
    x1 = self.dropout_ff(self.conv2(x1).transpose(-1, -2))
    output = x + x1

    return output, attention_

class Decoder(nn.Module):
  def __init__(self, decoder_layers, norm_layer=None, embedding_dropout=0.0):
    super().__init__()

    self.decoder_layers = nn.ModuleList(decoder_layers)
    self.norm_layer = norm_layer
    self.dropout = nn.Dropout(embedding_dropout)

  def forward(self, val_time_emb, space_emb, encoder_output, self_mask_seq=None, cross_mask_seq=None, output_cross_attention=False):
    x = self.dropout(val_time_emb) + self.dropout(space_emb)
    attentions = []
    for i, dec_layer in enumerate(self.decoder_layers):
      x, attention = dec_layer(x, encoder_output, self_mask_seq=self_mask_seq, cross_mask_seq=cross_mask_seq, cross_attention=output_cross_attention)
      attentions.append(attention)

    if self.norm_layer:
      x = self.norm_layer(x)

    return x, attentions
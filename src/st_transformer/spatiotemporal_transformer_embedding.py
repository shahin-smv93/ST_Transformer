# -*- coding: utf-8 -*-
"""Spatiotemporal_Transformer_embedding.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SxkD_WlEmYqDQCBoAHeZeDqN7sFjUjYx
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from einops import rearrange, repeat

from .spatiotemporal_transformer_time2vec import Time2Vec
from .spatiotemporal_transformer_extralayers import ConvBlock, Flatten

class Embedding(nn.Module):
  def __init__(self, d_y, d_x, d_model, time_emb_dim: int,
               downsample_convs=0, start_len = 0, pad_value=None,
               is_encoder: bool = True, data_dropout=None, max_seq_len=None,
               indicator_embedding: bool = True, time_embedding: bool = True):
    super().__init__()

    if data_dropout is None:
      self.data_drop = lambda y: y
    else:
      self.data_drop = data_dropout

    time_dim = time_emb_dim * d_x
    self.time_emb = Time2Vec(d_x, embed_dim=time_dim)

    self.max_seq_len = max_seq_len

    self.local_emb = nn.Embedding(max_seq_len, d_model)

    y_emb_inp_dim = 1

    self.val_time_emb = nn.Linear(y_emb_inp_dim + time_dim, d_model)


    self.space_emb = nn.Embedding(num_embeddings=d_y, embedding_dim=d_model)
    split_length_into = d_y

    self.start_len = start_len
    self.indicator_emb = nn.Embedding(num_embeddings=2, embedding_dim=d_model)

    self.downsize_convs = nn.ModuleList(
        [ConvBlock(split_length_into, d_model) for _ in range(downsample_convs)]
    )

    self.d_model = d_model
    self.pad_value = pad_value
    self.is_encoder = is_encoder
    self.indicator_embedding = indicator_embedding
    self.time_embedding = time_embedding

  def make_mask(self, y):
    if self.pad_value is None:
      return None
    else:
      return (y == self.pad_value).any(-1, keepdim=True)

  def forward(self, y:torch.Tensor, x:torch.Tensor):
    batch, length, dy = y.shape
    local_pos = repeat(torch.arange(length).to(x.device),
                       f"length -> {batch} ({dy} length)")

    local_emb = self.local_emb(local_pos)

    if self.time_embedding is None:
      x = torch.zeros_like(x)
    x = torch.nan_to_num(x)
    x = repeat(x, f"batch len x_dim -> batch ({dy} len) x_dim")
    time_emb = self.time_emb(x.float())

    y_true = torch.isnan(y)
    y = torch.nan_to_num(y)

    y_original = y.clone()
    y_original = Flatten(y_original)
    y = self.data_drop(y)
    y = Flatten(y)
    mask = self.make_mask(y)

    val_time_inp = torch.cat((time_emb, y), dim=-1)
    val_time_emb = self.val_time_emb(val_time_inp)

    if self.indicator_embedding:
      indicator = torch.ones((batch, length, dy)).long().to(x.device)
      if not self.is_encoder:
        indicator[:, self.start_len:, :] = 0
      indicator *= ~y_true
      indicator = rearrange(indicator, "batch seq_len dy -> batch (dy seq_len)")

      indicator *= (y == y_original).squeeze(-1)

      indicator_emb = self.indicator_emb(indicator)

    else:
      indicator_emb = 0.0


    val_time_emb = local_emb + val_time_emb + indicator_emb

    if self.is_encoder:
      for conv in self.downsize_convs:
        val_time_emb = conv(val_time_emb)
        length //= 2

    var_idx = repeat(torch.arange(dy).long().to(x.device), f"dy -> {batch} (dy {length})")
    var_idx_true = var_idx.clone()
    space_emb = self.space_emb(var_idx)

    return val_time_emb, space_emb, var_idx_true, mask
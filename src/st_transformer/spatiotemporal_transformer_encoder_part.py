# -*- coding: utf-8 -*-
"""Spatiotemporal_transformer_Encoder_part.ipynb

Automatically generated by Colab.

"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

from .spatiotemporal_transformer_extralayers import Flatten, localize, reverse_localize, Windowing, ReverseWindowing, SelfMaskingSeq, Normalization


class EncoderLayer(nn.Module):
  def __init__(self, d_model, d_y_context, local_attention, global_attention,
               d_q_k: int, d_v: int, n_heads: int, dropout_qkv: float,
               dropout_ff: float = 0.1, dropout_attn: float = 0.0, windows=1,
               window_offset=0, d_ff: int = None,
               normalization_type='batchnorm', activation='relu',
               ):
    super().__init__()
    if d_ff is None:
      d_ff = 4 * d_model
    else:
      d_ff = d_ff

    self.local_attention = local_attention

    self.global_attention = global_attention

    self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)
    self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)

    self.norm1 = Normalization(d_model=d_model, method=normalization_type)
    self.norm2 = Normalization(d_model=d_model, method=normalization_type)
    self.norm3 = Normalization(d_model=d_model, method=normalization_type)

    self.dropout_ff = nn.Dropout(dropout_ff)
    self.dropout_attn = nn.Dropout(dropout_attn)

    self.activation = F.relu if activation == 'relu' else F.gelu

    self.d_y_context = d_y_context
    self.windows = windows
    self.window_offset = window_offset

  def forward(self, x, self_mask_seq=None, output_attention=False):
    attention_ = None
    if self.local_attention:
      x1 = self.norm1(x)
      x1 = localize(x1, self.d_y_context)
      x1, _ = self.local_attention(x1, x1, x1, self_mask_seq, output_attention=False)
      x1 = reverse_localize(x1, self.d_y_context)
      x = x + self.dropout_attn(x1)

    if self.global_attention:
      x1 = self.norm2(x)
      x1 = Windowing(x1, num_windows=self.windows, window_offset=self.window_offset, divide_into=self.d_y_context)
      self_mask_seq = Windowing(self_mask_seq, num_windows=self.windows, window_offset=self.window_offset, divide_into=self.d_y_context)
      x1, attention_ = self.global_attention(x1, x1, x1, attention_mask=SelfMaskingSeq(self_mask_seq), output_attention=output_attention)
      x1 = ReverseWindowing(x1, num_windows=self.windows, window_offset=self.window_offset, divide_into=self.d_y_context)
      self_mask_seq = ReverseWindowing(self_mask_seq, num_windows=self.windows, window_offset=self.window_offset, divide_into=self.d_y_context)
      x = x + self.dropout_attn(x1)

    x1 = self.norm3(x)
    x1 = self.dropout_ff(self.activation(self.conv1(x1.transpose(-1, 1))))
    x1 = self.dropout_ff(self.conv2(x1).transpose(-1, 1))

    out = x + x1
    return out, attention_

class Encoder(nn.Module):
  def __init__(self, encoder_layers, conv_layers, norm_layer, dropout=0.0):
    super().__init__()

    self.encoder_layers = nn.ModuleList(encoder_layers)
    self.conv_layers = nn.ModuleList(conv_layers)
    self.norm_layer = norm_layer
    self.dropout = nn.Dropout(dropout)

  def forward(self, val_time_emb, space_emb, self_mask_seq=None, output_attention=False):
    x = self.dropout(val_time_emb) + self.dropout(space_emb)

    attentions = []
    for i, encoder_layer in enumerate(self.encoder_layers):
      x, attention = encoder_layer(x, self_mask_seq=self_mask_seq,
                                   output_attention=output_attention)
      if len(self.conv_layers) > i:
        if self.conv_layers[i]:
          x = self.conv_layers[i](x)
      attentions.append(attention)

    if self.norm_layer:
      x = self.norm_layer(x)

    return x, attentions
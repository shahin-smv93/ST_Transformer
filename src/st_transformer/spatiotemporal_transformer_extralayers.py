# -*- coding: utf-8 -*-
"""Spatiotemporal_Transformer_extraLayers.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kG0-cWoNlREg6KPKY_DJdo1zE-hyuP7V
"""

#!pip install einops

import torch
import torch.nn as nn
import torch.nn.functional as F
from einops import rearrange, repeat

class Normalization(nn.Module):
  def __init__(self, d_model=None, method='batchnorm'):
    super(Normalization, self).__init__()
    assert method in ['batchnorm', 'layernorm']

    assert d_model

    if method == 'batchnorm':
      self.norm = nn.BatchNorm1d(d_model)
    else:
      self.norm = nn.LayerNorm(d_model)

    self.method = method

  def forward(self, x):
    if self.method == 'batchnorm':
      return self.norm(x.permute(0, 2, 1)).permute(0, 2, 1)
    return self.norm(x)

class ConvBlock(nn.Module):
  def __init__(self, split_length_into, d_model, conv_kernel_size=3,
               conv_stride=1, pool=True, pool_kernel_size=3, pool_stride=2,
               activation="gelu"):
    super().__init__()
    self.split_length = split_length_into
    self.conv = nn.Conv1d(in_channels=d_model, out_channels=d_model,
                          kernel_size=conv_kernel_size, stride=conv_stride,
                          padding=1, padding_mode='circular')
    self.norm = nn.BatchNorm1d(d_model)
    if activation == "elu":
      self.activation = nn.ELU()
    elif activation == "relu":
      self.activation = nn.ReLU()
    elif activation == "gelu":
      self.activation = nn.GELU()
    else:
      raise ValueError("Invalid activation function")

    self.pool = (nn.MaxPool1d(kernel_size=pool_kernel_size, stride=pool_stride,
                              padding=1) if pool else lambda x: x)

  def conv_forward(self, x):
    x = self.conv(x)
    x = self.norm(x)
    x = self.activation(x)
    x = self.pool(x)
    return x

  def forward(self, x):
    x = rearrange(x, f"batch (sl len) d_model -> (batch sl) d_model len",
                  sl=self.split_length)
    x = self.conv_forward(x)
    x = rearrange(x, f"(batch sl) d_model len -> batch (sl len) d_model",
                  sl=self.split_length)
    return x

def Flatten(inp: torch.Tensor) -> torch.Tensor:
  output = rearrange(inp, "batch len dy -> batch (dy len) 1")
  return output

def localize(inp: torch.Tensor, variable_num: int) -> torch.Tensor:
  x = rearrange(inp,
                "batch (variable_num seq_len) dim -> (variable_num batch) seq_len dim",
                variable_num=variable_num)
  return x

def reverse_localize(inp: torch.Tensor, variable_num: int) -> torch.Tensor:
  x = rearrange(
      inp, "(variables batch) seq_len dim -> batch (variables seq_len) dim",
      variables=variable_num)
  return x

def Windowing(
    x: torch.Tensor, num_windows: int, window_offset: int, divide_into: int) -> torch.Tensor:
    if num_windows == 1 or x is None:
      return x

    x = x.view(int(x.size(0)), int(x.size(1) / divide_into), divide_into, int(x.size(2)))

    assert window_offset is not None
    assert num_windows > 1

    BATCH, l, _, n_features = x.shape
    window_len = l // 2
    shift_value = window_len // window_offset
    x = torch.roll(x, -shift_value, dims=1)

    x = rearrange(x, "batch (num_windows len) divided_into dim -> (batch num_windows) (divided_into len) dim", num_windows=num_windows)
    return x

def ReverseWindowing(
    x: torch.Tensor, num_windows: int, window_offset: int, divide_into: int) -> torch.Tensor:
    if num_windows == 1 or x is None:
      return x

    x = rearrange(x, "(BATCH num_windows) (divide_into len) dim -> BATCH (num_windows len) dy dim",
                  num_windows=num_windows, divide_into=divide_into)

    assert window_offset is not None
    assert num_windows > 1

    BATCH, l, _, n_features = x.shape
    window_len = l // 2
    shift_value = window_len // window_offset
    x = torch.roll(x, shift_value, dims=1)

    x = rearrange(x, "batch len divide_into dim -> batch (divide_into len) dim",
                divide_into=divide_into)
    return x

def SelfMaskingSeq(seq_mask: torch.Tensor):
  if seq_mask is None:
    return None

  batch, seq_len, dim = seq_mask.shape
  assert dim == 1

  mask_rows = repeat(seq_mask, f"batch len 1 -> batch {seq_len} len")
  mask_cols = repeat(seq_mask, f"batch len 1 -> batch len {seq_len}")
  mask = torch.max(mask_rows, mask_cols).bool()
  return mask

def CrossMaskingSeq(self_mask_seq: torch.Tensor, cross_mask_seq: torch.Tensor):
  if self_mask_seq is None:
    return None

  cross_batch, cross_seq_len, dim = cross_mask_seq.shape
  assert dim == 1
  self_batch, self_seq_len, dim = self_mask_seq.shape
  assert dim == 1
  assert self_batch == cross_batch

  mask_rows = repeat(cross_mask_seq, f"batch len 1 -> batch {self_seq_len} len")
  mask_cols = repeat(self_mask_seq, f"batch len 1 -> batch len {cross_seq_len}")
  mask = torch.max(mask_rows, mask_cols).bool()
  return mask

def PredRearrange(input: torch.Tensor, dy: int) -> torch.Tensor:
  output = rearrange(input, "batch (dy len) dim -> dim batch len dy", dy=dy)
  output = output.squeeze(0)
  return output
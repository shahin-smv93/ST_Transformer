# -*- coding: utf-8 -*-
"""SpatioTemporalPerformer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dpXpdCAJL_9S_V_R_KSajDMh6GHdJdL8
"""

from einops import rearrange, repeat
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import warnings

from .spatiotemporal_transformer_embedding import Embedding
from .spatiotemporal_transformer_extralayers import ConvBlock, Normalization, PredRearrange
from .spatiotemporal_transformer_attention import PerformerAttention, AttentionClass, create_performer_attention
from .spatiotemporal_transformer_encoder_part import EncoderLayer, Encoder
from .spatiotemporal_transformer_decoder_part import DecoderLayer, Decoder


class SpatioTemporalPerformer(nn.Module):
  def __init__(self, d_y_context: int, d_y_target: int, d_x: int, d_model: int,
               max_seq_len: int, d_q_k: int, d_v: int, n_heads:int,
               n_encoder_layers: int, n_decoder_layers: int, d_ff:int,
               start_len: int, time_emb_dim:int,
               downsample_convs_embedding: int = 0, downsample_convs_block: int = 0,
               pad_value: float = None,
               data_dropout_embedding: float = 0.1, dropout_attn: float = 0.0,
               dropout_qkv: float = 0.0, dropout_ff: float = 0.2,
               redraw_interval: int = 1000,
               time_embedding: bool = True, indicator_embedding: bool = True,
               performer_attention_kernel: str = "relu",
               windows: int = 1, use_shifted_time_window: bool = True,
               activation: str = 'gelu', normalization_type: str = 'batchnorm',
               final_norm: bool = True, verbose: bool = True, out_dim: int = None,
               device=torch.device("cuda:0"), autoregressive_training: bool = False
               ):
    super().__init__()
    if n_encoder_layers:
      assert downsample_convs_block <= n_encoder_layers -1

    split_length_into = d_y_context

    self.d_y_context = d_y_context
    self.d_y_target = d_y_target
    self.start_len = start_len
    self.pad_value = pad_value

    self.encoder_embedding = Embedding(d_y=d_y_context, d_x=d_x, d_model=d_model,
                                       time_emb_dim=time_emb_dim,
                                       downsample_convs=downsample_convs_embedding,
                                       pad_value=pad_value, start_len=start_len,
                                       data_dropout=None,
                                       time_embedding=time_embedding,
                                       indicator_embedding=indicator_embedding,
                                       max_seq_len=max_seq_len, is_encoder=True)


    self.decoder_embedding = Embedding(d_y=d_y_target, d_x=d_x, d_model=d_model,
                                       time_emb_dim=time_emb_dim,
                                       downsample_convs=downsample_convs_embedding,
                                       pad_value=pad_value, start_len=start_len,
                                       time_embedding=time_embedding,
                                       indicator_embedding=indicator_embedding,
                                       max_seq_len=max_seq_len,
                                       data_dropout=None,
                                       is_encoder=False)
    #check for dim_heads and d_q_k
    self.encoder = Encoder(
        encoder_layers=[
            EncoderLayer(
                d_model=d_model,
                d_y_context=d_y_context,
                local_attention=create_performer_attention(
                    d_model=d_model,
                    d_q_k=d_q_k,
                    d_v=d_v,
                    n_heads=n_heads,
                    dropout_qkv=dropout_qkv,
                    performer_attention_kernel=performer_attention_kernel,
                    redraw_interval=redraw_interval,
                    causal=False
                ),
                global_attention=create_performer_attention(
                    d_model=d_model,
                    d_q_k=d_q_k,
                    d_v=d_v,
                    n_heads=n_heads,
                    dropout_qkv=dropout_qkv,
                    performer_attention_kernel=performer_attention_kernel,
                    redraw_interval=redraw_interval,
                    causal=False
                ),
                d_q_k=d_q_k,
                d_v=d_v,
                n_heads=n_heads,
                dropout_qkv=dropout_qkv,
                dropout_ff=dropout_ff,
                dropout_attn=dropout_attn,
                windows=windows,
                window_offset=2 if use_shifted_time_window and (layer % 2 == 1) else 0,
                d_ff=d_ff,
                normalization_type=normalization_type,
                activation=activation,
            )
            for layer in range(n_encoder_layers)
        ],
        conv_layers=[
            ConvBlock(split_length_into=split_length_into, d_model=d_model)
            for layer in range(downsample_convs_block)
        ],
        norm_layer=Normalization(d_model=d_model, method=normalization_type) if final_norm else None,
        dropout=data_dropout_embedding
    )


    self.decoder=Decoder(
        decoder_layers=[
            DecoderLayer(
                d_model=d_model,
                d_y_target=d_y_target,
                d_y_context=d_y_context,
                local_self_attention=create_performer_attention(
                    d_model=d_model,
                    d_q_k=d_q_k,
                    d_v=d_v,
                    n_heads=n_heads,
                    dropout_qkv=dropout_qkv,
                    performer_attention_kernel=performer_attention_kernel,
                    redraw_interval=redraw_interval,
                    causal=autoregressive_training
                ),
                global_self_attention=create_performer_attention(
                    d_model=d_model,
                    d_q_k=d_q_k,
                    d_v=d_v,
                    n_heads=n_heads,
                    dropout_qkv=dropout_qkv,
                    performer_attention_kernel=performer_attention_kernel,
                    redraw_interval=redraw_interval,
                    causal=autoregressive_training
                ),
                local_cross_attention=create_performer_attention(
                    d_model=d_model,
                    d_q_k=d_q_k,
                    d_v=d_v,
                    n_heads=n_heads,
                    dropout_qkv=dropout_qkv,
                    performer_attention_kernel=performer_attention_kernel,
                    redraw_interval=redraw_interval,
                    causal=False
                ),
                global_cross_attention=create_performer_attention(
                    d_model=d_model,
                    d_q_k=d_q_k,
                    d_v=d_v,
                    n_heads=n_heads,
                    dropout_qkv=dropout_qkv,
                    performer_attention_kernel=performer_attention_kernel,
                    redraw_interval=redraw_interval,
                    causal=False
            ),
                d_q_k=d_q_k,
                d_v=d_v,
                n_heads=n_heads,
                dropout_qkv=dropout_qkv,
                dropout_ff=dropout_ff,
                dropout_attn=dropout_attn,
                windows=windows,
                window_offset=2 if use_shifted_time_window and (layer % 2 == 1) else 0,
                d_ff=d_ff,
                normalization_type=normalization_type,
                activation=activation,
            )
            for layer in range(n_decoder_layers)
        ],
        norm_layer=Normalization(d_model=d_model, method=normalization_type) if final_norm else None,
        embedding_dropout=data_dropout_embedding
    )

    msgprint = lambda x: print(x) if verbose else None
    msgprint(f"Time Embedding Dimension: {time_emb_dim}")
    msgprint(f"Time Embedding: {self.decoder_embedding.time_embedding}")
    msgprint(f"Indicator Embedding: {self.decoder_embedding.indicator_embedding}")
    msgprint(f"Padding Value: {self.decoder_embedding.pad_value}")


    if not out_dim:
      out_dim = 1

    self.forecaster = nn.Linear(d_model, out_dim, bias=True)

  def forward(self, enc_x, enc_y, dec_x, dec_y, output_attention=False):
    enc_val_time_emb, enc_space_emb, enc_var_idx, enc_mask_seq = self.encoder_embedding(
        y=enc_y, x=enc_x
    )

    enc_out, enc_attention = self.encoder(
        val_time_emb=enc_val_time_emb, space_emb=enc_space_emb,
        self_mask_seq=enc_mask_seq, output_attention=output_attention
    )

    dec_val_time_emb, dec_space_emb, _, dec_mask_seq = self.decoder_embedding(
        y=dec_y, x=dec_x
    )

    if enc_mask_seq is not None:
      cross_mask_seq = enc_mask_seq.clone()
    else:
      cross_mask_seq = enc_mask_seq

    dec_out, dec_attention = self.decoder(
        val_time_emb=dec_val_time_emb, space_emb=dec_space_emb,
        encoder_output=enc_out, self_mask_seq=dec_mask_seq,
        cross_mask_seq=cross_mask_seq, output_cross_attention=output_attention
    )

    forecast_out = self.forecaster(dec_out)

    forecast_out = PredRearrange(forecast_out, dy=self.d_y_target)

    forecast_out = forecast_out[:, self.start_len:, :]

    return (
        forecast_out,
        (enc_attention, dec_attention)
    )
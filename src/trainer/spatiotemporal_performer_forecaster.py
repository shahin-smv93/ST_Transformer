# -*- coding: utf-8 -*-
"""spatiotemporal_performer_forecaster_diff.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eBn9giTw62vMEx3EWObvMkF2Vbdk3e-j
"""

import sys
import pytorch_lightning as pl
from typing import Tuple
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from abc import ABC, abstractmethod
import torchmetrics
import torch.optim as optim
from torch.optim.lr_scheduler import ReduceLROnPlateau, _LRScheduler
import pytorch_lightning.callbacks as pl_callbacks
from transformers import get_linear_schedule_with_warmup
import math, random

torch.set_float32_matmul_precision('high')

sys.path.append('/content/drive/MyDrive/Spatiotemporal_Transformer/Scripts')
from spatiotemporalperformer import *

from spatiotemporal_transformer_extralayers import PredRearrange

from einops import rearrange


# with Huber Loss #
class Forecaster(pl.LightningModule, ABC):
    def __init__(self, d_x: int, d_y_context: int, d_y_target: int, learning_rate: float = 1e-3,
                 l2_regul_factor: float = 0, loss: str = 'mse', verbose=True,
                 linear_window: int = 0, linear_shared_weights: bool = False,
                 use_seasonal_decomp: bool = False, use_revin: bool = False,
                 num_training_steps: int = None, use_global_norm: bool = True, global_mean=None, global_std=None):
        super().__init__()

        self._inv_scaler = lambda x: x
        self.l2_regul_factor = l2_regul_factor
        assert loss in ['mse', 'mae', 'huber']
        self.loss = loss
        self.learning_rate = learning_rate
        self.use_revin = use_revin
        self.use_seasonal_decomp = use_seasonal_decomp
        self.num_training_steps = num_training_steps

        if linear_window:
            self.linear_model = LinearModel(
                linear_window, shared_weights=linear_shared_weights, d_yt=d_y_target
            )
        else:
            self.linear_model = lambda x, *args, **kwargs: 0.0

        if use_global_norm:
            assert global_mean is not None and global_std is not None
            self.norm_module = GlobalNorm(global_mean, global_std, affine=True)
        elif use_revin:
            assert d_y_context == d_y_target
            self.norm_module = RevIN(num_features=d_y_context)
            self.global_mean = None
            self.global_std = None
        else:
            self.norm_module = lambda x, *args, **kwargs: x
            self.global_mean = None
            self.global_std = None

        if use_seasonal_decomp:
            self.seasonal_decomp = SeriesDecomposition(kernel_size=25)
        else:
            self.seasonal_decomp = lambda x: (x, x.clone())



        self.d_x = d_x
        self.d_y_context = d_y_context
        self.d_y_target = d_y_target
        self.time_masked_idx = None
        self.null_value = None
        self.save_hyperparameters()

    @property
    @abstractmethod
    def train_step_forward_kwargs(self):
        return {}

    @property
    @abstractmethod
    def eval_step_forward_kwargs(self):
        return {}

    def loss_fn(self, ground_truth: torch.Tensor, prediction: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:
        if self.loss == 'mse':
            loss = (mask * (ground_truth - prediction) ** 2).sum() / max(mask.sum(), 1)
        elif self.loss == 'mae':
            loss = torch.abs(mask * (ground_truth - prediction)).sum() / max(mask.sum(), 1)
        elif self.loss == 'huber':
            loss = torch.nn.functional.smooth_l1_loss(ground_truth, prediction, reduction='none')
            loss = mask * loss
            loss = loss.sum() / max(mask.sum(), 1)
        else:
            raise ValueError(f'Unknown loss function: {self.loss}')
        return loss

    def forecast_loss(self, output: torch.Tensor, y_target: torch.Tensor, time_mask: int) -> Tuple[torch.Tensor]:
        if self.null_value is not None:
            null_mask_mat = y_target != self.null_value
        else:
            null_mask_mat = torch.ones_like(y_target)

        null_mask_mat *= ~torch.isnan(y_target)

        time_mask_mat = torch.ones_like(y_target)

        if time_mask is not None:
            time_mask_mat[:, time_mask:] = False

        full_mask = time_mask_mat * null_mask_mat
        forecast_loss = self.loss_fn(y_target, output, full_mask)
        return forecast_loss, full_mask

    def compute_loss(self, batch: Tuple[torch.Tensor], time_mask: int = None, forward_kwargs: dict = {}) -> Tuple[torch.Tensor]:
        x_context, y_context, x_target, y_target = batch

        outputs, *_ = self(x_context, y_context, x_target, y_target, **forward_kwargs)

        loss, mask = self.forecast_loss(outputs, y_target, time_mask)

        return loss, outputs, mask

    def predict(self, x_context, y_context, x_target, sample_preds: bool = False) -> torch.Tensor:
        original_device = y_context.device
        x_context = x_context.to(self.device).float()
        y_context = y_context.to(self.device).float()
        x_target = x_target.to(self.device).float()
        y_target = torch.zeros((x_target.shape[0], x_target.shape[1], self.d_y_target)).to(self.device).float()

        with torch.no_grad():
            norm_pred , *_ = self.forward(x_context, y_context, x_target, y_target, **self.eval_step_forward_kwargs)
        preds = norm_pred.to(original_device).float()
        return preds

    @abstractmethod
    def forward_model_pass(self, x_context: torch.Tensor, y_context: torch.Tensor, x_target: torch.Tensor, y_target: torch.Tensor, **forward_kwargs) -> Tuple[torch.Tensor]:
        raise NotImplemented

    def forward(self, x_context: torch.Tensor, y_context: torch.Tensor, x_target: torch.Tensor, y_target: torch.Tensor, **forward_kwargs) -> Tuple[torch.Tensor]:
        batch_size, pred_len, d_y_target = y_target.shape

        y_context = self.norm_module(y_context, mode="norm")

        seasonal_y_context, trend_y_context = self.seasonal_decomp(y_context)


        preds = self.forward_model_pass(x_context, seasonal_y_context, x_target, y_target, **forward_kwargs)
        baseline = self.linear_model(trend_y_context, pred_len=pred_len, d_yt=d_y_target)
        output = self.norm_module(preds + baseline, mode="denorm")

        return (output,)

    def _compute_stats(self, pred: torch.tensor, ground_truth: torch.tensor, mask: torch.tensor):
        pred = pred * mask
        ground_truth = ground_truth * mask

        adj = mask.mean().cpu().numpy() + 1e-5
        pred = pred.detach().cpu().numpy()
        true = ground_truth.detach().cpu().numpy()
        stats = {
            "mae": self.mae(true, pred) / adj,
            "mse": self.mse(true, pred) / adj,
        }
        return stats

    def step(self, batch: Tuple[torch.Tensor], train: bool = False):
        kwargs = (
            self.train_step_forward_kwargs if train else self.eval_step_forward_kwargs
        )
        time_mask = self.time_masked_idx if train else None

        loss, output, mask = self.compute_loss(batch, time_mask, kwargs)
        *_, y_target = batch
        stats = self._compute_stats(output, y_target, mask)
        stats['loss'] = loss
        section = 'train' if train else 'val'
        self.log(f"{section}/loss", stats["loss"], on_step=True, on_epoch=True, prog_bar=True)
        self.log(f"{section}/mae", stats['mae'], on_step=True, on_epoch=True)
        self.log(f"{section}/mse", stats['mse'], on_step=True, on_epoch=True)

        return {'loss': stats["loss"]}

    def training_step(self, batch, batch_idx):
        return self.step(batch, train=True)

    def validation_step(self, batch, batch_idx):
        return self.step(batch, train=False)

    def test_step(self, batch, batch_idx):
        return self.step(batch, train=False)


    def mae(self, true: np.ndarray, pred: np.ndarray) -> float:
        return np.mean(np.abs(true - pred))

    def mse(self, true: np.ndarray, pred: np.ndarray) -> float:
        return np.mean((true - pred) ** 2)


    def configure_optimizers(self):
        optimizer = torch.optim.Adam(
            self.parameters(), lr=self.learning_rate, weight_decay=self.l2_regul_factor
        )

        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer,
            mode='min',
            factor=0.2,
            patience=1,
            min_lr=1e-8,
            verbose=True
        )

        return {
            'optimizer': optimizer,
            'lr_scheduler': {
                'scheduler': scheduler,
                'monitor': 'val/loss',
                'interval': 'epoch',
                'frequency': 1,
            }
        }



### Scheduled sampling with real time information
### soft teacher forcing
class SpatioTemporalPerformer_Forecaster(Forecaster):
    def __init__(self,
                 d_y_context: int = 49,
                 d_y_target: int = 49,
                 d_x: int = 2,
                 max_seq_len: int = None,
                 start_len: int = 64,
                 d_model: int = None,
                 d_q_k: int = None,
                 d_v: int = None,
                 n_heads: int = 4,
                 n_encoder_layers: int = 2,
                 n_decoder_layers: int = 2,
                 d_ff: int = None,
                 time_emb_dim: int = None,
                 downsample_convs_embedding: int = 0,
                 downsample_convs_block: int = 0,
                 pad_value: float = None,
                 data_dropout_embedding: float = 0.1,
                 dropout_attn: float = 0.0,
                 dropout_qkv: float = 0.0,
                 dropout_ff: float = 0.2,
                 redraw_interval: int = 1000,
                 ortho_scaling: float = 0.0,
                 time_embedding: bool = True,
                 indicator_embedding: bool = True,
                 performer_attention_kernel: str = 'relu',
                 windows: int = 1,
                 use_shifted_time_window: bool = True,
                 activation: str = "gelu",
                 normalization_type: str = 'batchnorm',
                 final_norm: bool = True,
                 initial_lr: float = 1e-3,
                 peak_lr: float = 1e-8,
                 warmup_steps: int = 1000,
                 decay_factor: float = 0.8,
                 loss: str = 'mse',
                 l2_regul_factor: float = 1e-5,
                 verbose: bool = True,
                 linear_shared_weights: bool = False,
                 use_seasonal_decomp: bool = False,
                 linear_window: int = 0,
                 use_revin: bool = False,
                 use_global_norm: bool = True,
                 global_mean=None,
                 global_std=None,
                 num_training_steps=None,
                 # NEW parameters for scheduled sampling:
                 autoregressive_training: bool = False,
                 scheduled_sampling_k: float = 1000.0,
                 ):
        super().__init__(
            d_x=d_x,
            d_y_context=d_y_context,
            d_y_target=d_y_target,
            learning_rate=initial_lr,
            l2_regul_factor=l2_regul_factor,
            loss=loss,
            linear_shared_weights=linear_shared_weights,
            use_seasonal_decomp=use_seasonal_decomp,
            linear_window=linear_window,
            use_revin=use_revin,
            use_global_norm=use_global_norm,
            global_mean=global_mean,
            global_std=global_std,
            num_training_steps=num_training_steps,
        )

        self.spatiotemporalperformer = SpatioTemporalPerformer(
            d_y_context=d_y_context,
            d_y_target=d_y_target,
            d_x=d_x,
            d_model=d_model,
            d_q_k=d_q_k,
            d_v=d_v,
            n_heads=n_heads,
            n_encoder_layers=n_encoder_layers,
            n_decoder_layers=n_decoder_layers,
            start_len=start_len,
            max_seq_len=max_seq_len,
            time_emb_dim=time_emb_dim,
            d_ff=d_ff,
            downsample_convs_embedding=downsample_convs_embedding,
            downsample_convs_block=downsample_convs_block,
            pad_value=pad_value,
            data_dropout_embedding=data_dropout_embedding,
            dropout_attn=dropout_attn,
            dropout_qkv=dropout_qkv,
            dropout_ff=dropout_ff,
            redraw_interval=redraw_interval,
            time_embedding=time_embedding,
            indicator_embedding=indicator_embedding,
            performer_attention_kernel=performer_attention_kernel,
            windows=windows,
            use_shifted_time_window=use_shifted_time_window,
            activation=activation,
            normalization_type=normalization_type,
            final_norm=final_norm,
            device=self.device,
            autoregressive_training=autoregressive_training
        )
        self.initial_lr = initial_lr
        self.peak_lr = peak_lr
        self.warmup_steps = warmup_steps
        self.decay_factor = decay_factor
        self.start_len = start_len
        self.pad_value = pad_value

        # Save scheduled sampling parameters.
        self.autoregressive_training = autoregressive_training
        self.scheduled_sampling_k = scheduled_sampling_k

        self.save_hyperparameters()

        if verbose:
            print(f" ***** SpatioTemporalPerformer *****")
            print(f"\tModel Dimension: {d_model}")
            print(f"\tEncoder Layers: {n_encoder_layers}")
            print(f"\tDecoder Layers: {n_decoder_layers}")
            print(f"*********************************")

    @property
    def train_step_forward_kwargs(self):
        return {"output_attention": False}

    @property
    def eval_step_forward_kwargs(self):
        return {"output_attention": False}

    def get_teacher_forcing_ratio(self):
        # Inverse sigmoid decay: ratio = k / (k + exp(global_step/k))
        step = getattr(self, 'global_step', 0)
        k = self.scheduled_sampling_k
        ratio = k / (k + math.exp(step / k))
        return ratio

    def forward_model_pass(self, x_context: torch.Tensor, y_context: torch.Tensor,
                           x_target: torch.Tensor, y_target: torch.Tensor,
                           output_attention=False):
        """
        This method defines the forward pass of the model.
        We branch based on autoregressive_training flag.
          - If False, we use teacher forcing: the entire target sequence (including time info) is passed.
          - If True (scheduled sampling), we generate predictions step-by-step, feeding the model its own past predictions.
        """
        # Ensure y_context and y_target have 3 dimensions.
        if y_context.ndim == 2:
            y_context = y_context.unsqueeze(-1)
        if y_target.ndim == 2:
            y_target = y_target.unsqueeze(-1)

        # Teacher Forcing Branch (One-shot)
        if not self.autoregressive_training:
            encoder_x = x_context
            encoder_y = y_context
            decoder_x = x_target
            decoder_y = torch.zeros_like(y_target, device=self.device)

            if self.start_len > 0:
                decoder_x = torch.cat((x_context[:, -self.start_len:, :], decoder_x), dim=1)
                decoder_y = torch.cat((y_context[:, -self.start_len:, :], decoder_y), dim=1)

            forecast_output, attention = self.spatiotemporalperformer(
                enc_x=encoder_x,
                enc_y=encoder_y,
                dec_x=decoder_x,
                dec_y=decoder_y,
                output_attention=output_attention,
            )
            if output_attention:
                return forecast_output, None
            return forecast_output
        else:
            encoder_x = x_context
            encoder_y = y_context

            B = y_context.size(0)
            T = y_target.size(1)

            if self.start_len > 0:
                decoder_x = x_context[:, -self.start_len:, :].clone()
                decoder_y = y_context[:, -self.start_len:, :].clone()
            else:
                decoder_x = x_target.clone()
                decoder_y = x_context.new_zeros(B, 0, y_context.size(-1))

            predictions = []

            for t in range(T):
                next_time = x_target[:, t:t+1, :]
                decoder_x = torch.cat([decoder_x, next_time], dim=1)

                dummy_sensor = torch.zeros(B, 1, y_context.size(-1), device=self.device)
                current_dec_y = torch.cat([decoder_y, dummy_sensor], dim=1)

                output, _ = self.spatiotemporalperformer(
                    enc_x=encoder_x,
                    enc_y=encoder_y,
                    dec_x=decoder_x,
                    dec_y=current_dec_y,
                    output_attention=False,
                )
                pred = output[:, -1:, :].detach().clone()

                teacher_forcing_ratio = self.get_teacher_forcing_ratio()

                gt = y_target[:, t:t+1, :]

                combined = teacher_forcing_ratio * gt + (1 - teacher_forcing_ratio) * pred

                decoder_y = torch.cat([decoder_y, combined], dim=1)
                predictions.append(pred)

                if self.start_len > 0 and decoder_y.size(1) > self.start_len:
                    decoder_y = decoder_y[:, -self.start_len:, :].clone()
                    decoder_x = decoder_x[:, -self.start_len:, :].clone()

            forecast_output = torch.cat(predictions, dim=1)
            return forecast_output

    def configure_optimizers(self):
        optimizer = torch.optim.AdamW(
            self.parameters(), lr=self.initial_lr, weight_decay=self.l2_regul_factor
        )
        plateau_scheduler = ReduceLROnPlateau(
            optimizer,
            mode='min',
            min_lr=1e-8,
            patience=2,
            factor=self.decay_factor,
            verbose=True
        )
        plateau_config = {
            'scheduler': plateau_scheduler,
            'monitor': 'val/loss',
            'interval': 'epoch',
            'frequency': 1
        }
        num_training_steps = self.num_training_steps
        if num_training_steps is None:
            raise ValueError("num_training_steps must be provided")
        warmup_scheduler = get_linear_schedule_with_warmup(
            optimizer,
            num_warmup_steps=self.warmup_steps,
            num_training_steps=num_training_steps,
        )
        warmup_config = {
            'scheduler': warmup_scheduler,
            'interval': 'step',
        }
        return [optimizer], [warmup_config, plateau_config]

    def step(self, batch: Tuple[torch.Tensor], train: bool):
        kwargs = self.train_step_forward_kwargs if train else self.eval_step_forward_kwargs
        time_mask = self.time_masked_idx if train else None
        loss, output, mask = self.compute_loss(batch, time_mask, kwargs)
        *_, y_target = batch
        stats = self._compute_stats(output, y_target, mask)
        stats["loss"] = loss
        section = 'train' if train else 'val'
        self.log(f"{section}/loss", stats["loss"], on_step=True, on_epoch=True, prog_bar=True)
        self.log(f"{section}/mae", stats['mae'], on_step=True, on_epoch=True)
        self.log(f"{section}/mse", stats['mse'], on_step=True, on_epoch=True)
        return {'loss': stats["loss"]}

    def compute_loss(self, batch: Tuple[torch.Tensor], time_mask: int = None, forward_kwargs: dict = {}):
        x_context, y_context, x_target, y_target = batch
        outputs, *_ = self(x_context, y_context, x_target, y_target, **forward_kwargs)
        loss, mask = self.forecast_loss(outputs, y_target, time_mask)
        return loss, outputs, mask

